{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47386906",
   "metadata": {},
   "source": [
    "# Dev simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0853cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX 4000 Ada Generation Laptop GPU (UUID: GPU-2e6f8d70-aa40-c48f-e928-5c9265e18c5f)\n",
      "Python: 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:05:38) [MSC v.1929 64 bit (AMD64)]\n",
      "CUDA available: True\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Transformers: 4.51.3\n",
      "Accelerate: 1.4.0\n",
      "PEFT: 0.14.0\n",
      "matplotlib: 3.10.7\n",
      "== Environment ==\n",
      "{'python': '3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:05:38) [MSC v.1929 64 bit (AMD64)]', 'torch': '2.6.0+cu124', 'transformers': '4.51.3', 'accelerate': '1.4.0', 'peft': '0.14.0', 'cuda': '12.4', 'device_name': 'NVIDIA RTX 4000 Ada Generation Laptop GPU', 'platform': 'Windows-10-10.0.19045-SP0', 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "El sistema no puede encontrar la ruta especificada.\n"
     ]
    }
   ],
   "source": [
    "#@title ðŸ› ï¸ Setup\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import os, sys, random, numpy as np, torch, json, time, platform, math\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Qwen3 requires Transformers >= 4.51\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"uv\", \"pip -q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "except Exception:\n",
    "    get_ipython().run_line_magic(\"pip\", \"-q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "\n",
    "import transformers, datasets, peft, accelerate, matplotlib\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"Please connect a GPU (A100+ recommended).\"\n",
    "\n",
    "def print_header():\n",
    "    print(\"== Environment ==\")\n",
    "    print(dict(\n",
    "        python=sys.version,\n",
    "        torch=torch.__version__,\n",
    "        transformers=transformers.__version__,\n",
    "        accelerate=accelerate.__version__,\n",
    "        peft=peft.__version__,\n",
    "        cuda=torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        device_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        platform=platform.platform(),\n",
    "        seed=SEED\n",
    "    ))\n",
    "print_header()\n",
    "\n",
    "import os, sys, time, json, random, platform\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import re\n",
    "\n",
    "# --------------------------\n",
    "# Reproducibility & device\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"A CUDA GPU is required.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Environment ==\n",
      "{'python': '3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:05:38) [MSC v.1929 64 bit (AMD64)]', 'torch': '2.6.0+cu124', 'transformers': '4.51.3', 'accelerate': '1.4.0', 'peft': '0.14.0', 'cuda': '12.4', 'device_name': 'NVIDIA RTX 4000 Ada Generation Laptop GPU', 'platform': 'Windows-10-10.0.19045-SP0', 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Models\n",
    "    student_id: str = \"Qwen/Qwen3-0.6B-Base\"  # Modelo estudiante pequeÃ±o (0.6B parÃ¡metros) que serÃ¡ entrenado\n",
    "    # student_id: str = \"Qwen/Qwen3-1.7B-Base\"\n",
    "    teacher_id: str = \"Qwen/Qwen3-4B-Instruct-2507\"  # Modelo maestro grande (4B parÃ¡metros) que guÃ­a el entrenamiento\n",
    "\n",
    "    # Prompting\n",
    "    prompt_template: str = (\n",
    "        \"Solve step by step.\\n\"\n",
    "        \"Give ONLY ONE final numeric answer (no units), inside square brackets.\\n\"\n",
    "        \"Problem: {question}\\n\\nSolution:\"\n",
    "    )  # Plantilla de prompt para formatear las preguntas del dataset GSM8K\n",
    "    max_new_tokens: int = 256  # MÃ¡ximo nÃºmero de tokens a generar en cada respuesta (afecta longitud de salida)\n",
    "\n",
    "    # Generation temps\n",
    "    eval_temperature: float = 0.0   # Temperatura para evaluaciÃ³n (0.0 = greedy decoding, determinista)\n",
    "    train_temperature: float = 0.7  # Temperatura para generaciÃ³n durante entrenamiento (0.7 = sampling con diversidad)\n",
    "\n",
    "    # Training schedule\n",
    "    steps: int = 2  # NÃºmero total de iteraciones de entrenamiento\n",
    "    batch_prompts: int = 2  # NÃºmero de prompts diferentes a muestrear en cada step\n",
    "    samples_per_prompt: int = 3  # NÃºmero de muestras generadas por cada prompt (aumentar si tienes GPU grande)\n",
    "    lr: float = 1e-4  # Learning rate del optimizador AdamW\n",
    "    weight_decay: float = 0.0  # RegularizaciÃ³n L2 (weight decay) - 0.0 significa sin regularizaciÃ³n\n",
    "    grad_accum: int = 1  # NÃºmero de pasos para acumular gradientes antes de hacer update (Ãºtil para simular batches grandes)\n",
    "\n",
    "    # Micro-batching\n",
    "    student_mb: int = 2  # TamaÃ±o de micro-batch para procesar el modelo estudiante (reduce memoria GPU)\n",
    "\n",
    "    # Monitoring\n",
    "    log_every: int = 1  # Frecuencia de logging de mÃ©tricas (cada N steps)\n",
    "    val_every: int = 1  # Frecuencia de evaluaciÃ³n en validation set (cada N steps)\n",
    "    val_sample_n: int = 10  # NÃºmero de ejemplos del validation set a usar en cada evaluaciÃ³n\n",
    "    ema_momentum: float = 0.9  # Momentum para exponential moving average de mÃ©tricas (suaviza fluctuaciones)\n",
    "\n",
    "    # Validation size\n",
    "    val_rows: Optional[int] = 10  # TamaÃ±o del validation set (None = usar min(200, len(train)))\n",
    "\n",
    "    # Output dir\n",
    "    run_root: str = f\"./run_opd_{int(time.time())}\"  # Directorio raÃ­z donde se guardan logs y checkpoints\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.run_root, exist_ok=True)\n",
    "\n",
    "def print_env():\n",
    "    import transformers, accelerate, peft, matplotlib\n",
    "    print(\"== Environment ==\")\n",
    "    print({\n",
    "        \"python\": sys.version,\n",
    "        \"torch\": torch.__version__,\n",
    "        \"transformers\": transformers.__version__,\n",
    "        \"accelerate\": accelerate.__version__,\n",
    "        \"peft\": peft.__version__,\n",
    "        \"cuda\": torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        \"platform\": platform.platform(),\n",
    "        \"seed\": SEED\n",
    "    })\n",
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d148b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GSM8Kâ€¦\n",
      "Splits: 7463 train | 10 val | 1319 test\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Data: GSM8K\n",
    "# --------------------------\n",
    "def render_prompt(question: str) -> str:\n",
    "    return cfg.prompt_template.format(question=question)\n",
    "\n",
    "def parse_gold(answer_text: str) -> Optional[str]:\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", answer_text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", answer_text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "def parse_pred(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"\\[\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\]\", text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "print(\"Loading GSM8Kâ€¦\")\n",
    "ds_train_full = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "ds_test       = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "if cfg.val_rows is None:\n",
    "    val_rows = min(200, len(ds_train_full))\n",
    "else:\n",
    "    val_rows = min(cfg.val_rows, len(ds_train_full))\n",
    "\n",
    "ds_val   = ds_train_full.select(range(val_rows))\n",
    "ds_train = ds_train_full.select(range(val_rows, len(ds_train_full)))\n",
    "\n",
    "print(f\"Splits: {len(ds_train)} train | {len(ds_val)} val | {len(ds_test)} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30453571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Tokenizers & Models\n",
    "# --------------------------\n",
    "def load_tokenizers(student_id: str, teacher_id: str):\n",
    "    tok_s = AutoTokenizer.from_pretrained(student_id, use_fast=True)\n",
    "    tok_t = AutoTokenizer.from_pretrained(teacher_id, use_fast=True)\n",
    "    for tok in (tok_s, tok_t):\n",
    "        if tok.pad_token is None and tok.eos_token is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        # para hacer inferencia, mejor que todos queden completos a la derecha asi se arranca desde la misma posicion siempre para generacion\n",
    "        tok.padding_side = \"left\"  \n",
    "    return tok_s, tok_t\n",
    "\n",
    "def make_lora_student(model_id: str) -> torch.nn.Module:\n",
    "    # Cargar explÃ­citamente en CUDA\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\"  # Cambiado de \"auto\" a \"cuda\"\n",
    "    )\n",
    "    base.config.use_cache = False  # off for training\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    \n",
    "    # Verificar que estÃ¡ en CUDA\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"âœ“ Student model loaded on device: {device}\")\n",
    "    return model\n",
    "\n",
    "def load_teacher(model_id: str) -> torch.nn.Module:\n",
    "    # Cargar explÃ­citamente en CUDA\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\"  # Cambiado de \"auto\" a \"cuda\"\n",
    "    ).eval()\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    \n",
    "    # Verificar que estÃ¡ en CUDA\n",
    "    device = next(m.parameters()).device\n",
    "    print(f\"âœ“ Teacher model loaded on device: {device}\")\n",
    "    return m\n",
    "\n",
    "tok_s, tok_t = load_tokenizers(cfg.student_id, cfg.teacher_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d240b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0ba882",
   "metadata": {},
   "source": [
    "## Tests simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a551dc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para hacer un \"subsample\" de un Dataset y mantener el tipo Dataset (no dict):\n",
    "ds_sample = ds_train.select(range(10))\n",
    "ds_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb7433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Randy has 60 mango trees on his farm. He also has 5 less than half as many coconut trees as mango trees. How many trees does Randy have in all on his farm?',\n",
       " 'answer': \"Half of the number of Randy's mango trees is 60/2 = <<60/2=30>>30 trees.\\nSo Randy has 30 - 5 = <<30-5=25>>25 coconut trees.\\nTherefore, Randy has 60 + 25 = <<60+25=85>>85 treeson his farm.\\n#### 85\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "981dd0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve step by step.\n",
      "Give ONLY ONE final numeric answer (no units), inside square brackets.\n",
      "Problem: Randy has 60 mango trees on his farm. He also has 5 less than half as many coconut trees as mango trees. How many trees does Randy have in all on his farm?\n",
      "\n",
      "Solution:\n"
     ]
    }
   ],
   "source": [
    "# prompt renderizado\n",
    "print(render_prompt(ds_sample[2]['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c79bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "print(parse_gold(ds_sample[2]['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49ffa76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'82'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_solution = 'Creo que la solucion es [82]'\n",
    "parse_pred(sample_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db69e7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b37fbf2cf0e43aa8dc8a6192aa1e81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Teacher model loaded on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# student\n",
    "# model = make_lora_student(cfg.student_id)\n",
    "\n",
    "# teacher\n",
    "model = load_teacher(cfg.teacher_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2677eaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Prompt raw:\n",
      "Solve step by step.\n",
      "Give ONLY ONE final numeric answer (no units), inside square brackets.\n",
      "Problem: Randy has 60 mango trees on his farm. He also has 5 less than half as many coconut trees as mango trees. How many trees does Randy have in all on his farm?\n",
      "\n",
      "Solution:\n",
      "================================================================================\n",
      "Prompt raw + generacion del modelo:\n",
      "Solve step by step.\n",
      "Give ONLY ONE final numeric answer (no units), inside square brackets.\n",
      "Problem: Randy has 60 mango trees on his farm. He also has 5 less than half as many coconut trees as mango trees. How many trees does Randy have in all on his farm?\n",
      "\n",
      "Solution: 60 + 5/2 - 5 = 60 + 2.5 - 5 = 57.5\n",
      "Randy has 57.5 trees on his farm.\n"
     ]
    }
   ],
   "source": [
    "# pruebas simples con distintos prompts para ver como siguen los modelos segun el prompt crudo o template o sistema, etc que le pasamos\n",
    "prompt = 'My name is Qwen and my biggest problem is that'\n",
    "prompt = \"What do you know about Y-TEC?\"\n",
    "prompt = ds_sample[2]['question']\n",
    "prompt = render_prompt(ds_sample[2]['question']) # como lo hacemos mas arriba\n",
    "# prompt = f\"\"\"Answer the following question: {ds_sample[2]['question']}\"\"\"\n",
    "# prompt = f\"\"\"User: Answer the following question: {ds_sample[2]['question']}\n",
    "# Assistant: \"\"\"\n",
    "# prompt = f\"\"\"Question: {ds_sample[2]['question']}\n",
    "# Answer: \"\"\"\n",
    "\n",
    "# ### formatear con el chat template de qwen (esto formatea automÃ¡ticamente con <|im_start|> y <|im_end|>)\n",
    "# messages = [{\"role\": \"user\", \"content\": ds_sample[2]['question']}]\n",
    "# prompt = tok_t.apply_chat_template(\n",
    "#     messages, \n",
    "#     tokenize=False,  # Si quieres solo el texto formateado\n",
    "#     add_generation_prompt=True  # Agrega el prompt para que el modelo responda\n",
    "# )\n",
    "# ### \n",
    "\n",
    "k=4\n",
    "temperature=0.0\n",
    "max_new_tokens=100\n",
    "\n",
    "model.eval(); model.config.use_cache = True\n",
    "model_device = next(model.parameters()).device\n",
    "\n",
    "enc = tok_s([prompt], return_tensors=\"pt\").to(model_device)\n",
    "\n",
    "gen_kwargs = dict(max_new_tokens=max_new_tokens, use_cache=True, pad_token_id=tok_s.pad_token_id, eos_token_id=tok_s.eos_token_id)\n",
    "if temperature and temperature > 0.0:\n",
    "        gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "out = model.generate(**enc, **gen_kwargs)\n",
    "\n",
    "text = tok_s.decode(out[0], skip_special_tokens=True)\n",
    "print(\"=\"*80); print(f\"Prompt raw:\\n{prompt}\"); print(\"=\"*80); print(f\"Prompt raw + generacion del modelo:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5795352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold: 85\n",
      "Pred: 57.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST: evaluar\n",
    "\n",
    "gold = parse_gold(ds_sample[2]['answer'])\n",
    "print(f\"Gold: {gold}\")\n",
    "\n",
    "pred = parse_pred(text) # esto es lo que sale del test de arriba\n",
    "print(f\"Pred: {pred}\") # el parsing de la pred primero busca en brackets y si no hay, busca numeros\n",
    "\n",
    "int(pred == gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed8381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b80f231",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc298c",
   "metadata": {},
   "source": [
    "### Orquestrator -> json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c78696e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Orchestrator: GraphSpec (2 agents) by TEXT\n",
    "# =========================\n",
    "from typing import Dict, Any, List, Optional\n",
    "import json, re\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# 1) LLM client (usando modelo local)\n",
    "# -------------------------\n",
    "# Variables globales para el modelo y tokenizer (se asignan despuÃ©s de cargar el modelo)\n",
    "_local_model = None\n",
    "_local_tokenizer = None\n",
    "\n",
    "def set_local_llm(model, tokenizer):\n",
    "    \"\"\"Configura el modelo y tokenizer locales para usar en llm_complete.\"\"\"\n",
    "    global _local_model, _local_tokenizer\n",
    "    _local_model = model\n",
    "    _local_tokenizer = tokenizer\n",
    "\n",
    "@torch.no_grad()\n",
    "def llm_complete(messages: List[Dict[str, str]], model: Optional[str] = None, temperature: float = 0.2, max_tokens: int = 800) -> str:\n",
    "    \"\"\"\n",
    "    Usa el modelo local cargado en memoria para generar respuestas.\n",
    "    Convierte los mensajes del formato chat a un prompt usando el chat template del tokenizer.\n",
    "    \"\"\"\n",
    "    global _local_model, _local_tokenizer\n",
    "    \n",
    "    if _local_model is None or _local_tokenizer is None:\n",
    "        raise ValueError(\"Modelo local no configurado. Llama a set_local_llm(model, tokenizer) primero.\")\n",
    "    \n",
    "    # Preparar el modelo para inferencia\n",
    "    _local_model.eval()\n",
    "    _local_model.config.use_cache = True\n",
    "    \n",
    "    # Obtener el dispositivo del modelo\n",
    "    model_device = next(_local_model.parameters()).device\n",
    "    \n",
    "    # Convertir mensajes a formato de prompt usando el chat template\n",
    "    # Si el tokenizer tiene chat template, lo usamos; si no, construimos el prompt manualmente\n",
    "    try:\n",
    "        # Intentar usar el chat template del tokenizer\n",
    "        prompt = _local_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    except (AttributeError, ValueError, TypeError):\n",
    "        # Fallback: construir el prompt manualmente si no hay chat template\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"system\":\n",
    "                prompt_parts.append(f\"System: {content}\\n\")\n",
    "            elif role == \"user\":\n",
    "                prompt_parts.append(f\"User: {content}\\n\")\n",
    "            elif role == \"assistant\":\n",
    "                prompt_parts.append(f\"Assistant: {content}\\n\")\n",
    "        prompt = \"\".join(prompt_parts) + \"Assistant: \"\n",
    "    \n",
    "    # Mostrar el prompt final\n",
    "    print(\"\\nPROMPT FINAL (input al modelo):\")\n",
    "    print(prompt)\n",
    "    print()\n",
    "    \n",
    "    # Tokenizar el prompt\n",
    "    enc = _local_tokenizer([prompt], return_tensors=\"pt\").to(model_device)\n",
    "    \n",
    "    # Configurar parÃ¡metros de generaciÃ³n\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_tokens,\n",
    "        use_cache=True,\n",
    "        pad_token_id=_local_tokenizer.pad_token_id,\n",
    "        eos_token_id=_local_tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    if temperature > 0.0:\n",
    "        gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "    else:\n",
    "        gen_kwargs.update(do_sample=False)\n",
    "    \n",
    "    # Generar respuesta\n",
    "    with torch.no_grad():\n",
    "        out = _local_model.generate(**enc, **gen_kwargs)\n",
    "    \n",
    "    # Decodificar solo los tokens generados (sin el prompt)\n",
    "    generated_tokens = out[0][enc['input_ids'].shape[1]:]\n",
    "    text = _local_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# -------------------------\n",
    "# 2) JSON extractor robusto\n",
    "# -------------------------\n",
    "def extract_json_from_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Busca el primer bloque ```json ... ``` o el primer {...} parseable.\n",
    "    \"\"\"\n",
    "    print(f\"{\"=\"*50}\\nRESPUESTA CRUDA DEL LLM:\")\n",
    "    print(text)\n",
    "    print()\n",
    "    \n",
    "    # Try fenced block\n",
    "    fence = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.S)\n",
    "    if fence:\n",
    "        json_str = fence.group(1)\n",
    "        # print(\"âœ“ Encontrado bloque ```json```\")\n",
    "        # print(\"ðŸ“‹ JSON EXTRAÃDO:\")\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Error al parsear JSON: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Try first balanced-ish { ... }\n",
    "    brace = re.search(r\"(\\{.*\\})\", text, re.S)\n",
    "    if brace:\n",
    "        json_str = brace.group(1)\n",
    "        # print(\"âœ“ Encontrado bloque {...}\")\n",
    "        # print(\"ðŸ“‹ JSON EXTRAÃDO:\")\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Error al parsear JSON: {e}\")\n",
    "            raise\n",
    "    \n",
    "    print(\"âŒ No se encontrÃ³ JSON vÃ¡lido en el texto\")\n",
    "    raise ValueError(\"No se pudo extraer JSON del texto del orquestador.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Schema + validador mÃ­nimo (dos agentes)\n",
    "# -------------------------\n",
    "def validate_graph_spec(spec: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    ValidaciÃ³n para estructura simplificada (solo agents con system prompts).\n",
    "    Estructura esperada:\n",
    "    {\n",
    "      \"agents\": [\n",
    "        {\n",
    "          \"id\": \"agentA\",\n",
    "          \"type\": \"llm\",\n",
    "          \"system_prompt\": \"<string>\"\n",
    "        },\n",
    "        {\n",
    "          \"id\": \"agentB\",\n",
    "          \"type\": \"llm\",\n",
    "          \"system_prompt\": \"<string>\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    assert isinstance(spec, dict), \"spec debe ser dict\"\n",
    "    assert \"agents\" in spec and isinstance(spec[\"agents\"], list), \"agents faltante\"\n",
    "    assert len(spec[\"agents\"]) == 2, \"v0: exactamente 2 agentes\"\n",
    "    ids = [a.get(\"id\") for a in spec[\"agents\"]]\n",
    "    assert all(isinstance(i, str) for i in ids), \"ids invÃ¡lidos\"\n",
    "    assert len(set(ids)) == 2, \"ids de agentes deben ser Ãºnicos\"\n",
    "    assert ids == [\"agentA\", \"agentB\"], \"ids deben ser exactamente 'agentA' y 'agentB'\"\n",
    "\n",
    "    for a in spec[\"agents\"]:\n",
    "        assert a.get(\"type\") == \"llm\", \"v0: type debe ser 'llm'\"\n",
    "        assert isinstance(a.get(\"system_prompt\"), str) and len(a[\"system_prompt\"]) > 0, \"system_prompt vacÃ­o\"\n",
    "\n",
    "# -------------------------\n",
    "# 4) Prompt del orquestador (simplificado)\n",
    "# -------------------------\n",
    "ORCH_SYSTEM = (\n",
    "    \"You are designing a two-agent system to solve math word problems -> Agent A and Agent B.\"\n",
    "    \"The information flow is automatic: problem â†’ Agent A â†’ (problem + draft) â†’ Agent B â†’ final answer.\\n\\n\"\n",
    "    \"Your task: Design appropriate system prompts for each agent that will guide them in their roles.\\n\"\n",
    "    \"Output ONLY a valid JSON object in a ```json block with this exact structure:\\n\"\n",
    "    '{\\n'\n",
    "    '  \"agents\": [\\n'\n",
    "    '    {\"id\": \"agentA\", \"type\": \"llm\", \"system_prompt\": \"<prompt for planning/drafting>\"},\\n'\n",
    "    '    {\"id\": \"agentB\", \"type\": \"llm\", \"system_prompt\": \"<prompt for solving/finalizing>\"}\\n'\n",
    "    '  ]\\n'\n",
    "    '}\\n\\n'\n",
    "    \"Do not include anything else, only the structure above.\\n\"\n",
    "    \"The last agent is free to use any reasoning or format (including step-by-step) when working, but when giving the final_answer, it must write ONLY ONE final numeric answer (no units) inside square brackets.\\n\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) FunciÃ³n principal del orquestador\n",
    "# -------------------------\n",
    "def orchestrator_generate_graph(problem_text: str, model: Optional[str] = None, temperature: float = 0.2, max_tokens: int = 800) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Llama al LLM con un prompt simple: system prompt + pregunta.\n",
    "    Devuelve el dict ya parseado y validado.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ORCH_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": problem_text},\n",
    "    ]\n",
    "    \n",
    "    raw = llm_complete(messages, model=model, temperature=temperature, max_tokens=max_tokens)\n",
    "    spec = extract_json_from_text(raw)\n",
    "    validate_graph_spec(spec)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dc09ae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Modelo local configurado para el orquestador\n"
     ]
    }
   ],
   "source": [
    "# Configurar el modelo y tokenizer locales para el orquestador\n",
    "# AsegÃºrate de que 'model' y 'tok_s' estÃ©n cargados antes de ejecutar esto\n",
    "# Si usas el teacher model, cambia 'model' por 'teacher' y 'tok_s' por 'tok_t'\n",
    "set_local_llm(model, tok_s)\n",
    "print(\"âœ“ Modelo local configurado para el orquestador\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f092cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?\n",
      "----------------------------------------\n",
      "A pound of cream cheese cost $10 / 2 = $<<10/2=5>>5.\n",
      "A pack of cold cuts cost $10 x 2 = $<<10*2=20>>20.\n",
      "Jasper spent $10 + $5 + $20 = $<<10+5+20=35>>35 on the ingredients.\n",
      "#### 35\n"
     ]
    }
   ],
   "source": [
    "example_number = 3\n",
    "print(ds_sample[example_number]['question'])\n",
    "print('-'*40)\n",
    "print(ds_sample[example_number]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9ab1a9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROMPT FINAL (input al modelo):\n",
      "<|im_start|>system\n",
      "You are designing a two-agent system to solve math word problems -> Agent A and Agent B.The information flow is automatic: problem â†’ Agent A â†’ (problem + draft) â†’ Agent B â†’ final answer.\n",
      "\n",
      "Your task: Design appropriate system prompts for each agent that will guide them in their roles.\n",
      "Output ONLY a valid JSON object in a ```json block with this exact structure:\n",
      "{\n",
      "  \"agents\": [\n",
      "    {\"id\": \"agentA\", \"type\": \"llm\", \"system_prompt\": \"<prompt for planning/drafting>\"},\n",
      "    {\"id\": \"agentB\", \"type\": \"llm\", \"system_prompt\": \"<prompt for solving/finalizing>\"}\n",
      "  ]\n",
      "}\n",
      "\n",
      "Do not include anything else, only the structure above.\n",
      "The last agent is free to use any reasoning or format (including step-by-step) when working, but when giving the final_answer, it must write ONLY ONE final numeric answer (no units) inside square brackets.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "RESPUESTA CRUDA DEL LLM:\n",
      "```json\n",
      "{\n",
      "  \"agents\": [\n",
      "    {\n",
      "      \"id\": \"agentA\",\n",
      "      \"type\": \"llm\",\n",
      "      \"system_prompt\": \"You are tasked with creating a draft solution to a math word problem. First, identify all the given quantities and their relationships. Extract the prices of each ingredient based on the described relationships. Sum up the total cost of all ingredients in a clear, step-by-step draft. Do not provide a final answer yet; only generate a draft for the next agent to refine.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"agentB\",\n",
      "      \"type\": \"llm\",\n",
      "      \"system_prompt\": \"You receive a draft solution from Agent A. Review the draft, verify all calculations, and ensure the final answer is accurate. Perform the final computation to determine the total cost of the ingredients. Output only one numeric value in square brackets, with no units or explanations. For example: [45]\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.3\n",
    "max_tokens = 500\n",
    "\n",
    "mas_json = orchestrator_generate_graph(ds_sample[example_number]['question'], model, temperature, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6caadcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agents': [{'id': 'agentA',\n",
       "   'type': 'llm',\n",
       "   'system_prompt': 'You are tasked with creating a draft solution to a math word problem. First, identify all the given quantities and their relationships. Extract the prices of each ingredient based on the described relationships. Sum up the total cost of all ingredients in a clear, step-by-step draft. Do not provide a final answer yet; only generate a draft for the next agent to refine.'},\n",
       "  {'id': 'agentB',\n",
       "   'type': 'llm',\n",
       "   'system_prompt': 'You receive a draft solution from Agent A. Review the draft, verify all calculations, and ensure the final answer is accurate. Perform the final computation to determine the total cost of the ingredients. Output only one numeric value in square brackets, with no units or explanations. For example: [45]'}]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mas_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3e559",
   "metadata": {},
   "source": [
    "### Crear MAS a partir del json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5bb72ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Runner: ejecuta el GraphSpec (2 agentes en serie)\n",
    "# =========================\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Reusa llm_complete del bloque anterior.\n",
    "\n",
    "def _render_user_inputs(input_map: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render simple para el mensaje del 'user' a cada agente.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for k, v in input_map.items():\n",
    "        lines.append(f\"{k.upper()}: {v}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _agent_call_llm(agent: Dict[str, Any], input_payload: Dict[str, Any], temperature: float = 0.2, max_tokens: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    Construye mensajes (system + user) y llama al LLM.\n",
    "    \"\"\"\n",
    "    sys_prompt = agent[\"system_prompt\"]\n",
    "    user_text = _render_user_inputs(input_payload)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_text}\n",
    "    ]\n",
    "    return llm_complete(messages, model=None, temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "def run_two_agent_graph(graph_spec: Dict[str, Any], problem_text: str, temperature: float = 0.2, max_tokens: int = 512) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ejecuta el grafo de 2 agentes en serie con flujo fijo.\n",
    "    Flujo: problem â†’ Agent A â†’ (problem + draft) â†’ Agent B â†’ final answer\n",
    "\n",
    "    Args:\n",
    "        graph_spec: Dict con solo \"agents\" (lista de 2 agentes con id, type, system_prompt)\n",
    "        problem_text: El problema matemÃ¡tico a resolver\n",
    "        temperature: Temperatura para generaciÃ³n\n",
    "        max_tokens: MÃ¡ximo de tokens a generar\n",
    "\n",
    "    Returns:\n",
    "        Dict con 'trace' y 'final_text'\n",
    "    \"\"\"\n",
    "    # ValidaciÃ³n\n",
    "    assert isinstance(graph_spec, dict), \"graph_spec debe ser dict\"\n",
    "    assert \"agents\" in graph_spec and isinstance(graph_spec[\"agents\"], list), \"agents faltante\"\n",
    "    assert len(graph_spec[\"agents\"]) == 2, \"v0: exactamente 2 agentes\"\n",
    "\n",
    "    agent_a = graph_spec[\"agents\"][0]\n",
    "    agent_b = graph_spec[\"agents\"][1]\n",
    "    assert agent_a[\"id\"] == \"agentA\", \"Primer agente debe ser agentA\"\n",
    "    assert agent_b[\"id\"] == \"agentB\", \"Segundo agente debe ser agentB\"\n",
    "\n",
    "    trace: Dict[str, Any] = {\"agents\": {}}\n",
    "\n",
    "    # Ejecutar Agent A: recibe solo el problema\n",
    "    print(\"\\nðŸ¤– Ejecutando Agent A...\")\n",
    "    payload_a = {\"problem\": problem_text}\n",
    "    draft = _agent_call_llm(agent_a, payload_a, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(f\"Respuesta cruda Agent A:\\n{draft}\\n\")  # <-- agregado para ver salida cruda\n",
    "\n",
    "    trace[\"agents\"][\"agentA\"] = {\n",
    "        \"system_prompt\": agent_a[\"system_prompt\"],\n",
    "        \"input_payload\": payload_a,\n",
    "        \"output_text\": draft\n",
    "    }\n",
    "\n",
    "    # Ejecutar Agent B: recibe problema + draft de A\n",
    "    print(\"\\nðŸ¤– Ejecutando Agent B...\")\n",
    "    payload_b = {\n",
    "        \"problem\": problem_text,\n",
    "        \"draft\": draft\n",
    "    }\n",
    "    final_answer = _agent_call_llm(agent_b, payload_b, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(f\"Respuesta cruda Agent B:\\n{final_answer}\\n\")  # <-- agregado para ver salida cruda\n",
    "\n",
    "    trace[\"agents\"][\"agentB\"] = {\n",
    "        \"system_prompt\": agent_b[\"system_prompt\"],\n",
    "        \"input_payload\": payload_b,\n",
    "        \"output_text\": final_answer\n",
    "    }\n",
    "\n",
    "    return {\"trace\": trace, \"final_text\": final_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Ejecutando Agent A...\n",
      "\n",
      "PROMPT FINAL (input al modelo):\n",
      "<|im_start|>system\n",
      "You are tasked with creating a draft solution to a math word problem. First, identify all the given quantities and their relationships. Extract the prices of each ingredient based on the described relationships. Sum up the total cost of all ingredients in a clear, step-by-step draft. Do not provide a final answer yet; only generate a draft for the next agent to refine.<|im_end|>\n",
      "<|im_start|>user\n",
      "PROBLEM: Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Respuesta cruda Agent A:\n",
      "Let's break down the problem step by step:\n",
      "\n",
      "1. **Cheddar cheese**:  \n",
      "   Jasper buys 2 pounds of cheddar cheese for $10.  \n",
      "   So, the total cost of cheddar cheese = **$10**\n",
      "\n",
      "2. **Cream cheese**:  \n",
      "   It costs **half the price of the cheddar cheese**.  \n",
      "   The cheddar cheese costs $10, so cream cheese costs:  \n",
      "   $ \\frac{1}{2} \\times 10 = $5 $\n",
      "\n",
      "3. **Cold cuts**:  \n",
      "   It costs **twice the price of the cheddar cheese**.  \n",
      "   So, cold cuts cost:  \n",
      "   $ 2 \\times 10 = $20 $\n",
      "\n",
      "Now, add up all the costs:\n",
      "\n",
      "- Cheddar cheese: $10  \n",
      "- Cream cheese: $5  \n",
      "- Cold cuts: $20  \n",
      "\n",
      "Total cost = $10 + $5 + $20 = **$35**\n",
      "\n",
      "âœ… Final answer: **$35**\n",
      "\n",
      "\n",
      "ðŸ¤– Ejecutando Agent B...\n",
      "\n",
      "PROMPT FINAL (input al modelo):\n",
      "<|im_start|>system\n",
      "You receive a draft solution from Agent A. Review the draft, verify all calculations, and ensure the final answer is accurate. Perform the final computation to determine the total cost of the ingredients. Output only one numeric value in square brackets, with no units or explanations. For example: [45]<|im_end|>\n",
      "<|im_start|>user\n",
      "PROBLEM: Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?\n",
      "DRAFT: Let's break down the problem step by step:\n",
      "\n",
      "1. **Cheddar cheese**:  \n",
      "   Jasper buys 2 pounds of cheddar cheese for $10.  \n",
      "   So, the total cost of cheddar cheese = **$10**\n",
      "\n",
      "2. **Cream cheese**:  \n",
      "   It costs **half the price of the cheddar cheese**.  \n",
      "   The cheddar cheese costs $10, so cream cheese costs:  \n",
      "   $ \\frac{1}{2} \\times 10 = $5 $\n",
      "\n",
      "3. **Cold cuts**:  \n",
      "   It costs **twice the price of the cheddar cheese**.  \n",
      "   So, cold cuts cost:  \n",
      "   $ 2 \\times 10 = $20 $\n",
      "\n",
      "Now, add up all the costs:\n",
      "\n",
      "- Cheddar cheese: $10  \n",
      "- Cream cheese: $5  \n",
      "- Cold cuts: $20  \n",
      "\n",
      "Total cost = $10 + $5 + $20 = **$35**\n",
      "\n",
      "âœ… Final answer: **$35**<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Respuesta cruda Agent B:\n",
      "[35]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trace': {'agents': {'agentA': {'system_prompt': 'You are tasked with creating a draft solution to a math word problem. First, identify all the given quantities and their relationships. Extract the prices of each ingredient based on the described relationships. Sum up the total cost of all ingredients in a clear, step-by-step draft. Do not provide a final answer yet; only generate a draft for the next agent to refine.',\n",
       "    'input_payload': {'problem': 'Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?'},\n",
       "    'output_text': \"Let's break down the problem step by step:\\n\\n1. **Cheddar cheese**:  \\n   Jasper buys 2 pounds of cheddar cheese for $10.  \\n   So, the total cost of cheddar cheese = **$10**\\n\\n2. **Cream cheese**:  \\n   It costs **half the price of the cheddar cheese**.  \\n   The cheddar cheese costs $10, so cream cheese costs:  \\n   $ \\\\frac{1}{2} \\\\times 10 = $5 $\\n\\n3. **Cold cuts**:  \\n   It costs **twice the price of the cheddar cheese**.  \\n   So, cold cuts cost:  \\n   $ 2 \\\\times 10 = $20 $\\n\\nNow, add up all the costs:\\n\\n- Cheddar cheese: $10  \\n- Cream cheese: $5  \\n- Cold cuts: $20  \\n\\nTotal cost = $10 + $5 + $20 = **$35**\\n\\nâœ… Final answer: **$35**\"},\n",
       "   'agentB': {'system_prompt': 'You receive a draft solution from Agent A. Review the draft, verify all calculations, and ensure the final answer is accurate. Perform the final computation to determine the total cost of the ingredients. Output only one numeric value in square brackets, with no units or explanations. For example: [45]',\n",
       "    'input_payload': {'problem': 'Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?',\n",
       "     'draft': \"Let's break down the problem step by step:\\n\\n1. **Cheddar cheese**:  \\n   Jasper buys 2 pounds of cheddar cheese for $10.  \\n   So, the total cost of cheddar cheese = **$10**\\n\\n2. **Cream cheese**:  \\n   It costs **half the price of the cheddar cheese**.  \\n   The cheddar cheese costs $10, so cream cheese costs:  \\n   $ \\\\frac{1}{2} \\\\times 10 = $5 $\\n\\n3. **Cold cuts**:  \\n   It costs **twice the price of the cheddar cheese**.  \\n   So, cold cuts cost:  \\n   $ 2 \\\\times 10 = $20 $\\n\\nNow, add up all the costs:\\n\\n- Cheddar cheese: $10  \\n- Cream cheese: $5  \\n- Cold cuts: $20  \\n\\nTotal cost = $10 + $5 + $20 = **$35**\\n\\nâœ… Final answer: **$35**\"},\n",
       "    'output_text': '[35]'}}},\n",
       " 'final_text': '[35]'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run_two_agent_graph(\n",
    "    graph_spec=mas_json,\n",
    "    problem_text=ds_sample[example_number]['question'],\n",
    "    temperature=0.2,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "991c8ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output raw: [35]\n",
      "==============================\n",
      "Output parsed: 35\n",
      "==============================\n",
      "Gold: 35\n",
      "==============================\n",
      "Correct: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output raw: {result['final_text']}\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Output parsed: {parse_pred(result['final_text'])}\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Gold: {parse_gold(ds_sample[example_number]['answer'])}\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Correct: {int(parse_pred(result['final_text'])) == int(parse_gold(ds_sample[example_number]['answer']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823dbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25c7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62927c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4fda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e06c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d819712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96ab9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18dcc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af122b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036606c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed56ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
